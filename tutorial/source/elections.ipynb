{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting a US presidential election using Bayesian optimal polling\n",
    "\n",
    "In this tutorial, we explore the use of optimal experimental design techniques to create an optimal polling strategy to predict the outcome of the US presidential election. In a [previous tutorial](working_memory.ipynb), we explored the use of Bayesian optimal experimental design to learn the working memory capacity of a single person. Here, we apply the same concepts to study a whole country.\n",
    "\n",
    "To begin, we need a Bayesian model of the winner of the election `w`, as well as the outcome `y` of any poll we may plan to conduct. The experimental design is the number of people $n_i$ to poll in each state. In our model we will introduce an additional latent variable `alpha` (which is defined below). We will take historical election data 19760-2012 as our prior and the 2016 election as our test set: we imagine that we are conducting polling just before the 2016 election.\n",
    "\n",
    "## Choosing a prior\n",
    "For each of the 50 states plus DC we define \n",
    "\n",
    "$$ \\alpha_i = \\text{logit }\\mathbb{P}(\\text{a random voter in state } i \\text{ votes Democrat in the 2016 election}) $$\n",
    "\n",
    "and we assume all other voters vote Republican. Right before the election, the value of $\\alpha$ is unknown and we wish to estimate it by conducting a poll with $n_i$ people in state $i$ for $i=1, ..., 51$ . The winner $w$ of the election is decided by the Electoral College system. The number of electoral college votes gained by the Democrats in state $i$ is\n",
    "$$\n",
    "e_i =  \\begin{cases}\n",
    "k_i \\text{ if } \\alpha_i > \\frac{1}{2} \\\\\n",
    "0 \\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "(this is a rough approximation of the true system). All other electoral college votes go to the Republicans. Here $k_i$ is the number of electoral college votes alloted to state $i$, which are listed in the following data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Electoral college votes\n",
      "State                         \n",
      "AL                           9\n",
      "AK                           3\n",
      "AZ                          11\n",
      "AR                           6\n",
      "CA                          55\n",
      "CO                           9\n",
      "CT                           7\n",
      "DE                           3\n",
      "DC                           3\n",
      "FL                          29\n",
      "GA                          16\n",
      "HI                           4\n",
      "ID                           4\n",
      "IL                          20\n",
      "IN                          11\n",
      "IA                           6\n",
      "KS                           6\n",
      "KY                           8\n",
      "LA                           8\n",
      "ME                           4\n",
      "MD                          10\n",
      "MA                          11\n",
      "MI                          16\n",
      "MN                          10\n",
      "MS                           6\n",
      "MO                          10\n",
      "MT                           3\n",
      "NE                           5\n",
      "NV                           6\n",
      "NH                           4\n",
      "NJ                          14\n",
      "NM                           5\n",
      "NY                          29\n",
      "NC                          15\n",
      "ND                           3\n",
      "OH                          18\n",
      "OK                           7\n",
      "OR                           7\n",
      "PA                          20\n",
      "RI                           4\n",
      "SC                           9\n",
      "SD                           3\n",
      "TN                          11\n",
      "TX                          38\n",
      "UT                           6\n",
      "VT                           3\n",
      "VA                          13\n",
      "WA                          12\n",
      "WV                           5\n",
      "WI                          10\n",
      "WY                           3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "electoral_college_votes = pd.read_pickle(\"electoral_college_votes.pickle\")\n",
    "print(electoral_college_votes)\n",
    "ec_votes_tensor = torch.tensor(electoral_college_votes.values, dtype=torch.float).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The winner $w$ of the election is\n",
    "\n",
    "$$ w = \\begin{cases}\n",
    "\\text{Democrats if } \\sum_i e_i > \\frac{1}{2}\\sum_i k_i  \\\\\n",
    "\\text{Republicans otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in polling strategies that will help us predict $w$, rather than predicting the more complex state-by-state results $\\alpha$.\n",
    "\n",
    "To set up a fully Bayesian model, we need a prior for $\\alpha$. We will base the prior on the outcome of some historical presidential elections. Specifically, we'll use the following dataset of state-by-state election results for the presidential elections 1976-2012 inclusive. Note that votes for parties other than Democrats and Republicans have been ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          1976                1980                1984                1988  \\\n",
      "      Democrat Republican Democrat Republican Democrat Republican Democrat   \n",
      "State                                                                        \n",
      "AL      659170     504070   636730     654192   551899     872849   549506   \n",
      "AK       44058      71555    41842      86112    62007     138377    72584   \n",
      "AZ      295602     418642   246843     529688   333854     681416   454029   \n",
      "AR      499614     268753   398041     403164   338646     534774   349237   \n",
      "CA     3742284    3882244  3083661    4524858  3922519    5467009  4702233   \n",
      "CO      460353     584367   367973     652264   454974     821818   621453   \n",
      "CT      647895     719261   541732     677210   569597     890877   676584   \n",
      "DE      122596     109831   105754     111252   101656     152190   108647   \n",
      "DC      137818      27873   130231      23313   180408      29009   159407   \n",
      "FL     1636000    1469531  1419475    2046951  1448816    2730350  1656701   \n",
      "GA      979409     483743   890733     654168   706628    1068722   714792   \n",
      "HI      147375     140003   135879     130112   147154     185050   192364   \n",
      "ID      126549     204151   110192     290699   108510     297523   147272   \n",
      "IL     2271295    2364269  1981413    2358049  2086499    2707103  2215940   \n",
      "IN     1014714    1183958   844197    1255656   841481    1377230   860643   \n",
      "IA      619931     632863   508672     676026   605620     703088   670557   \n",
      "KS      430421     502752   326150     566812   333149     677296   422636   \n",
      "KY      615717     531852   616417     635274   539589     822782   580368   \n",
      "LA      661365     587446   708453     792853   651586    1037299   734281   \n",
      "ME      232279     236320   220974     238522   214515     336500   243569   \n",
      "MD      759612     672661   726161     680606   787935     879918   826304   \n",
      "MA     1429475    1030276  1053802    1057631  1239606    1310936  1401406   \n",
      "MI     1696714    1893742  1661532    1915225  1529638    2251571  1675783   \n",
      "MN     1070440     819395   954174     873241  1036364    1032603  1109471   \n",
      "MS      381309     366846   429281     441089   352192     581477   363921   \n",
      "MO      998387     927443   931182    1074181   848583    1274188  1001619   \n",
      "MT      149259     173703   118032     206814   146742     232450   168936   \n",
      "NE      233692     359705   166851     419937   187866     460054   259646   \n",
      "NV       92479     101273    66666     155017    91655     188770   132738   \n",
      "NH      147635     185935   108864     221705   120395     267051   163696   \n",
      "NJ     1444653    1509688  1147364    1546557  1261323    1933630  1320352   \n",
      "NM      201148     211419   167826     250779   201769     307101   244497   \n",
      "NY     3389558    3100791  2728372    2893831  3119609    3664763  3347882   \n",
      "NC      927365     741960   875635     915018   824287    1346481   890167   \n",
      "ND      136078     153470    79189     193695   104429     200336   127739   \n",
      "OH     2011621    2000505  1752414    2206545  1825440    2678560  1939629   \n",
      "OK      532442     545708   402026     695570   385080     861530   483423   \n",
      "OR      490407     492120   456890     571044   536479     685700   616206   \n",
      "PA     2328677    2205604  1937540    2261872  2228131    2584323  2194944   \n",
      "RI      227636     181249   198342     154793   197106     212080   225123   \n",
      "SC      450825     346140   427560     441207   344470     615539   370554   \n",
      "SD      147068     151505   103855     198343   116113     200267   145560   \n",
      "TN      825879     633969   783051     787761   711714     990212   679794   \n",
      "TX     2082319    1953300  1881147    2510705  1949276    3433428  2352748   \n",
      "UT      182110     337908   124266     439687   155369     469105   207343   \n",
      "VT       81044     102085    81891      94598    95730     135865   115775   \n",
      "VA      813896     836554   752174     989609   796250    1337078   859799   \n",
      "WA      717323     777732   650193     865244   807352    1051670   933516   \n",
      "WV      435914     314760   367462     334206   328125     405483   341016   \n",
      "WI     1040232    1004987   981584    1088845   995847    1198800  1126794   \n",
      "WY       62239      92717    49427     110700    53370     133241    67113   \n",
      "\n",
      "                     1992                1996                2000             \\\n",
      "      Republican Democrat Republican Democrat Republican Democrat Republican   \n",
      "State                                                                          \n",
      "AL        815576   690080     804283   662165     769044   692611     941173   \n",
      "AK        119251    78294     102000    80380     122746    79004     167398   \n",
      "AZ        702541   543050     572086   653288     622073   685341     781652   \n",
      "AR        466578   505823     337324   475171     325416   422768     472940   \n",
      "CA       5054917  5121325    3630574  5119835    3828380  5861203    4567429   \n",
      "CO        728177   629681     562850   671152     691848   738227     883748   \n",
      "CT        750241   682318     578313   735740     483109   816015     561094   \n",
      "DE        139639   126054     102313   140355      99062   180068     137288   \n",
      "DC         27590   192619      20698   158220      17339   171923      18073   \n",
      "FL       2618885  2072698    2173310  2546870    2244536  2912253    2912790   \n",
      "GA       1081331  1008966     995252  1053849    1080843  1116230    1419720   \n",
      "HI        158625   179310     136822   205012     113943   205286     137845   \n",
      "ID        253881   137013     202645   165443     256595   138637     336937   \n",
      "IL       2310939  2453350    1734096  2341744    1587021  2589026    2019421   \n",
      "IN       1297763   848420     989375   887424    1006693   901980    1245836   \n",
      "IA        545355   586353     504891   620258     492644   638517     634373   \n",
      "KS        554049   390434     449951   387659     583245   399276     622332   \n",
      "KY        734281   665104     617178   636614     623283   638898     872492   \n",
      "LA        883702   815971     733386   927837     712586   792344     927871   \n",
      "ME        307131   263420     206504   312788     186378   319951     286616   \n",
      "MD        876167   988571     707094   966207     681530  1145782     813797   \n",
      "MA       1194644  1318662     805049  1571763     718107  1616487     878502   \n",
      "MI       1965486  1871182    1554940  1989653    1481212  2170418    1953139   \n",
      "MN        962337  1020997     747841  1120438     766476  1168266    1109659   \n",
      "MS        557890   400258     487793   394022     439838   404614     572844   \n",
      "MO       1084953  1053873     811159  1025935     890016  1111138    1189924   \n",
      "MT        190412   154507     144207   167922     179652   137126     240178   \n",
      "NE        398447   217344     344346   236761     363467   231780     433862   \n",
      "NV        206040   189148     175828   203974     199244   279978     301575   \n",
      "NH        281537   209040     202484   246214     196532   266348     273559   \n",
      "NJ       1743192  1436206    1356865  1652329    1103078  1788850    1284173   \n",
      "NM        270341   261617     212824   273495     232751   286783     286417   \n",
      "NY       3081871  3444450    2346649  3756177    1933492  4107697    2403374   \n",
      "NC       1237258  1114042    1134661  1107849    1225938  1257692    1631163   \n",
      "ND        166559    99168     136244   106905     125050    95284     174852   \n",
      "OH       2416549  1984942    1894310  2148222    1859883  2186190    2351209   \n",
      "OK        678367   473066     592929   488105     582315   474276     744337   \n",
      "OR        560126   621314     475757   649641     538152   720342     713577   \n",
      "PA       2300087  2239164    1791841  2215819    1801169  2485967    2281127   \n",
      "RI        177761   213299     131601   233050     104683   249508     130555   \n",
      "SC        606443   479514     577507   504051     573458   565561     785937   \n",
      "SD        165415   124888     136718   139333     150543   118804     190700   \n",
      "TN        947233   933521     841300   909146     863530   981720    1061949   \n",
      "TX       3036829  2281815    2496071  2459683    2736167  2433746    3799639   \n",
      "UT        428442   183429     322632   221633     361911   203053     515096   \n",
      "VT        124331   133592      88122   137894      80352   149022     119775   \n",
      "VA       1309162  1038650    1150517  1091060    1138350  1217290    1437490   \n",
      "WA        903835   993037     731234  1123323     840712  1247652    1108864   \n",
      "WV        310065   331001     241974   327812     233946   295497     336475   \n",
      "WI       1047499  1041066     930855  1071971     845029  1242987    1237279   \n",
      "WY        106867    68160      79347    77934     105388    60481     147947   \n",
      "\n",
      "          2004                2008                2012             \n",
      "      Democrat Republican Democrat Republican Democrat Republican  \n",
      "State                                                              \n",
      "AL      693933    1176394   813479    1266546   795696    1255925  \n",
      "AK      111025     190889   123594     193841   122640     164676  \n",
      "AZ      893524    1104294  1034707    1230111  1025232    1233654  \n",
      "AR      469953     572898   422310     638017   394409     647744  \n",
      "CA     6745485    5509826  8274473    5011781  7854285    4839958  \n",
      "CO     1001732    1101255  1288633    1073629  1323101    1185243  \n",
      "CT      857488     693826   997772     629428   905083     634892  \n",
      "DE      200152     171660   255459     152374   242584     165484  \n",
      "DC      202970      21256   245800      17367   267070      21381  \n",
      "FL     3583544    3964522  4282074    4045624  4237756    4163447  \n",
      "GA     1366149    1914254  1844123    2048759  1773827    2078688  \n",
      "HI      231708     194191   325871     120566   306658     121015  \n",
      "ID      181098     409235   236440     403012   212787     420911  \n",
      "IL     2891550    2345946  3419348    2031179  3019512    2135216  \n",
      "IN      969011    1479438  1374039    1345648  1152887    1420543  \n",
      "IA      741898     751957   828940     682379   822544     730617  \n",
      "KS      434993     736456   514765     699655   440726     692634  \n",
      "KY      712733    1069439   751985    1048462   679370    1087190  \n",
      "LA      820299    1102169   782989    1148275   809141    1152262  \n",
      "ME      396842     330201   421923     295273   401306     292276  \n",
      "MD     1334493    1024703  1629467     959862  1677844     971869  \n",
      "MA     1803800    1071109  1904097    1108854  1921290    1188314  \n",
      "MI     2479183    2313746  2872579    2048639  2564569    2115256  \n",
      "MN     1445014    1346695  1573354    1275409  1546167    1320225  \n",
      "MS      458094     684981   554662     724597   562949     710746  \n",
      "MO     1259171    1455713  1441911    1445814  1223796    1482440  \n",
      "MT      173710     266063   231667     242763   201839     267928  \n",
      "NE      254328     512814   333319     452979   302081     475064  \n",
      "NV      397190     418690   533736     412827   531373     463567  \n",
      "NH      340511     331237   384826     316534   369561     329918  \n",
      "NJ     1911430    1670003  2215422    1613207  2125101    1477568  \n",
      "NM      370942     376930   472422     346832   415335     335788  \n",
      "NY     4314280    2962567  4804945    2752771  4485741    2490431  \n",
      "NC     1525849    1961166  2142651    2128474  2178391    2270395  \n",
      "ND      111052     196651   141278     168601   124827     188163  \n",
      "OH     2741167    2859768  2940044    2677820  2827709    2661437  \n",
      "OK      503966     959792   502496     960165   443547     891325  \n",
      "OR      943163     866831  1037291     738475   970488     754175  \n",
      "PA     2938095    2793847  3276363    2655885  2990274    2680434  \n",
      "RI      259765     169046   296571     165391   279677     157204  \n",
      "SC      661699     937974   862449    1034896   865941    1071645  \n",
      "SD      149244     232584   170924     203054   145039     210610  \n",
      "TN     1036477    1384375  1087437    1479178   960709    1462330  \n",
      "TX     2832704    4526917  3528633    4479328  3308124    4569843  \n",
      "UT      241199     663742   327670     596030   251813     740600  \n",
      "VT      184067     121180   219262      98974   199239      92698  \n",
      "VA     1454742    1716959  1959532    1725005  1971820    1822522  \n",
      "WA     1510201    1304894  1750848    1229216  1755396    1290670  \n",
      "WV      326541     423778   303857     397466   238269     417655  \n",
      "WI     1489504    1478120  1677211    1262393  1620985    1407966  \n",
      "WY       70776     167629    82868     164958    69286     170962  \n"
     ]
    }
   ],
   "source": [
    "frame = pd.read_pickle(\"us_presidential_election_data_historical.pickle\")\n",
    "print(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this data alone, we will base our prior mean for $\\alpha$ solely on the 2012 election. Specifically, we'll choose a prior mean as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4564, -0.2947, -0.1851, -0.4961,  0.4842,  0.1100,  0.3546,  0.3825,\n",
      "         2.5250,  0.0177, -0.1586,  0.9298, -0.6821,  0.3465, -0.2088,  0.1185,\n",
      "        -0.4521, -0.4702, -0.3535,  0.3170,  0.5460,  0.4805,  0.1926,  0.1580,\n",
      "        -0.2331, -0.1917, -0.2832, -0.4528,  0.1365,  0.1135,  0.3634,  0.2126,\n",
      "         0.5884, -0.0414, -0.4104,  0.0606, -0.6979,  0.2522,  0.1094,  0.5761,\n",
      "        -0.2131, -0.3730, -0.4201, -0.3231, -1.0788,  0.7652,  0.0787,  0.3075,\n",
      "        -0.5613,  0.1409, -0.9032])\n"
     ]
    }
   ],
   "source": [
    "results_2012 = torch.tensor(frame[2012].values, dtype=torch.float)\n",
    "prior_mean = torch.log(results_2012[..., 0] / results_2012[..., 1])\n",
    "print(prior_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prior distribution for $\\alpha$ will be a multivariate Normal with mean `prior_mean`. The only thing left to decide upon is the covariance matrix.\n",
    "\n",
    "*Aside*: The prior covariance is important in a number of ways. If we allow too much variance, the prior will be uncertain about the outcome in every state, and require polling everywhere. If we allow too little variance, we may be caught off-guard by an unexpected electoral outcome. If we assume states are independent, then we will not be able to pool information across states; but assume too much correlation and we could too faithfully base predictions about one state from poll results in another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the prior covariance by taking the empirical covariance from the elections 1976 - 2012 and adding a small value `0.01` to the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2 * torch.arange(10)\n",
    "as_tensor = torch.tensor(frame.values, dtype=torch.float)\n",
    "logits = torch.log(as_tensor[..., idx] / as_tensor[..., idx + 1]).transpose(0, 1)\n",
    "mean = logits.mean(0)\n",
    "sample_covariance = (1/(logits.shape[0] - 1)) * (\n",
    "    (logits.unsqueeze(-1) - mean) * (logits.unsqueeze(-2) - mean)\n",
    ").sum(0)\n",
    "prior_covariance = sample_covariance + 0.01 * torch.eye(sample_covariance.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "We are now in a position to define our model. At a high-level the model works as follows:\n",
    " - $\\alpha$ is multivariate Normal\n",
    " - $w$ is a deterministic function of $\\alpha$\n",
    " - $y_i$ is Binomial($n_i$, sigmoid($\\alpha_i$)) so we are assuming that people respond to the in exactly the same way that they will vote on election day\n",
    " \n",
    "In Pyro, this model looks as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "def model(polling_allocation):\n",
    "    # This allows us to run many copies of the model in parallel\n",
    "    with pyro.plate_stack(\"plate_stack\", polling_allocation.shape[:-1]):\n",
    "        # Begin by sampling alpha\n",
    "        alpha = pyro.sample(\"alpha\", dist.MultivariateNormal(\n",
    "            prior_mean, covariance_matrix=prior_covariance))\n",
    "        \n",
    "        # Sample y conditional on alpha\n",
    "        poll_results = pyro.sample(\"y\", dist.Binomial(\n",
    "            polling_allocation, logits=alpha).to_event(1))\n",
    "        \n",
    "        # Now compute w according to the (approximate) electoral college formula\n",
    "        dem_win_state = (alpha > 0.).float()\n",
    "        dem_electoral_college_votes = ec_votes_tensor * dem_win_state\n",
    "        dem_win = (dem_electoral_college_votes.sum(-1) / ec_votes_tensor.sum(-1) > .5).float()\n",
    "        pyro.sample(\"w\", dist.Delta(dem_win))\n",
    "        \n",
    "        return poll_results, dem_win, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the prior\n",
    "\n",
    "Before we go any further, we're going to study the model to check it matches with our intuition about US presidential elections.\n",
    "\n",
    "First of all, let's look at an upper and lower confidence limit for the proportion of voters who will vote Democrat in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Lower confidence limit  Upper confidence limit\n",
      "State                                                \n",
      "AL                   0.272258                0.517586\n",
      "AK                   0.330472                0.529117\n",
      "AZ                   0.321011                0.593634\n",
      "AR                   0.214348                0.576079\n",
      "CA                   0.458618                0.756616\n",
      "CO                   0.389236                0.661634\n",
      "CT                   0.428616                0.730398\n",
      "DE                   0.446085                0.727396\n",
      "DC                   0.857844                0.962764\n",
      "FL                   0.369840                0.638367\n",
      "GA                   0.295009                0.635060\n",
      "HI                   0.533767                0.848690\n",
      "ID                   0.236235                0.452438\n",
      "IL                   0.448084                0.711251\n",
      "IN                   0.356999                0.542615\n",
      "IA                   0.434126                0.622944\n",
      "KS                   0.298247                0.487878\n",
      "KY                   0.277151                0.504567\n",
      "LA                   0.295862                0.539931\n",
      "ME                   0.428039                0.715838\n",
      "MD                   0.514727                0.737528\n",
      "MA                   0.477512                0.740954\n",
      "MI                   0.425321                0.665121\n",
      "MN                   0.462240                0.614740\n",
      "MS                   0.347001                0.541403\n",
      "MO                   0.352351                0.556078\n",
      "MT                   0.315901                0.551363\n",
      "NE                   0.286474                0.501766\n",
      "NV                   0.350878                0.708520\n",
      "NH                   0.341560                0.707504\n",
      "NJ                   0.433178                0.730220\n",
      "NM                   0.422208                0.676761\n",
      "NY                   0.494502                0.768327\n",
      "NC                   0.380829                0.599479\n",
      "ND                   0.279116                0.531979\n",
      "OH                   0.419891                0.609312\n",
      "OK                   0.224875                0.460499\n",
      "OR                   0.454386                0.665369\n",
      "PA                   0.443535                0.609592\n",
      "RI                   0.508820                0.753413\n",
      "SC                   0.327229                0.573092\n",
      "SD                   0.298888                0.526622\n",
      "TN                   0.286040                0.518608\n",
      "TX                   0.321245                0.525443\n",
      "UT                   0.161671                0.374794\n",
      "VT                   0.482017                0.832337\n",
      "VA                   0.408404                0.629026\n",
      "WA                   0.450881                0.692573\n",
      "WV                   0.232679                0.517676\n",
      "WI                   0.449771                0.618544\n",
      "WY                   0.183177                0.422766\n"
     ]
    }
   ],
   "source": [
    "std = prior_covariance.diag().sqrt()\n",
    "ci = pd.DataFrame({\"State\": frame.index,\n",
    "                   \"Lower confidence limit\": torch.sigmoid(prior_mean - 1.96 * std), \n",
    "                   \"Upper confidence limit\": torch.sigmoid(prior_mean + 1.96 * std)}\n",
    "                 ).set_index(\"State\")\n",
    "print(ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior on $\\alpha$ implicitly defines our prior on `w`. We can investigate this prior by simulating many times from the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probability of Dem win 0.680649995803833\n"
     ]
    }
   ],
   "source": [
    "_, dem_wins, alpha_samples = model(torch.ones(100000, 51))\n",
    "prior_w_prob = dem_wins.float().mean()\n",
    "print(\"Prior probability of Dem win\", prior_w_prob.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our prior is based on 2012 and the Democrats won in 2012, it makes sense that we would favour a Democrat win in 2016 (this is before we have seen *any* polling data or incorporated any other information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also investigate which states, a priori, are most marginal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Democrat win probability\n",
      "State                          \n",
      "FL                      0.52452\n",
      "NC                      0.42641\n",
      "NH                      0.61436\n",
      "OH                      0.62144\n",
      "VA                      0.63478\n",
      "NV                      0.63827\n",
      "CO                      0.65118\n",
      "GA                      0.32955\n",
      "IA                      0.72790\n",
      "PA                      0.73690\n",
      "AZ                      0.26135\n",
      "MI                      0.77755\n",
      "NM                      0.78658\n",
      "WI                      0.78989\n",
      "SC                      0.20398\n",
      "MO                      0.18224\n",
      "MN                      0.84255\n",
      "ME                      0.84803\n",
      "IN                      0.13995\n",
      "CT                      0.86286\n",
      "NJ                      0.87157\n",
      "MT                      0.12685\n",
      "OR                      0.87394\n",
      "MS                      0.12551\n",
      "WA                      0.88400\n",
      "AR                      0.11370\n",
      "IL                      0.89054\n",
      "DE                      0.89512\n",
      "LA                      0.08739\n",
      "AK                      0.08051\n",
      "CA                      0.92894\n",
      "ND                      0.06616\n",
      "TX                      0.06611\n",
      "SD                      0.06156\n",
      "MA                      0.95166\n",
      "TN                      0.04773\n",
      "AL                      0.04424\n",
      "WV                      0.03997\n",
      "VT                      0.96440\n",
      "KY                      0.02900\n",
      "NY                      0.97109\n",
      "NE                      0.02664\n",
      "RI                      0.98211\n",
      "MD                      0.98603\n",
      "KS                      0.01329\n",
      "HI                      0.98878\n",
      "OK                      0.00547\n",
      "ID                      0.00321\n",
      "WY                      0.00114\n",
      "UT                      0.00009\n",
      "DC                      1.00000\n"
     ]
    }
   ],
   "source": [
    "dem_prob = (alpha_samples > 0.).float().mean(0)\n",
    "marginal = torch.argsort((dem_prob - .5).abs()).numpy()\n",
    "prior_prob_dem = pd.DataFrame({\"State\": frame.index[marginal],\n",
    "                               \"Democrat win probability\": dem_prob.numpy()[marginal]}\n",
    "                             ).set_index('State')\n",
    "print(prior_prob_dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sanity check, and seems to accord with our intuitions. Florida is frequently an important swing state and is top of our list of marginal states under the prior. We can also see states such as Pennsylvania and Wisconsin near the top of the list -- we know that these were instrumental in the 2016 election. (This kind of posthoc analysis is, of course, only engaged in by people of low statistical morals.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we take a closer look at our prior covariance. Specifically, we examine states that we expect to be more or less correlated. Let's begin by looking at states in New England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State        ME        VT        NH        MA        RI        CT\n",
      "State                                                            \n",
      "ME     1.000000  0.817323  0.857351  0.800276  0.822024  0.825383\n",
      "VT     0.817323  1.000000  0.834723  0.716342  0.754026  0.844140\n",
      "NH     0.857351  0.834723  1.000000  0.871370  0.803803  0.873496\n",
      "MA     0.800276  0.716342  0.871370  1.000000  0.813665  0.835148\n",
      "RI     0.822024  0.754026  0.803803  0.813665  1.000000  0.849644\n",
      "CT     0.825383  0.844140  0.873496  0.835148  0.849644  1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def correlation(cov):\n",
    "    return cov / np.sqrt(np.expand_dims(np.diag(cov.values), 0) * np.expand_dims(np.diag(cov.values), 1))\n",
    "                \n",
    "\n",
    "new_england_states = ['ME', 'VT', 'NH', 'MA', 'RI', 'CT']\n",
    "cov_as_frame = pd.DataFrame(prior_covariance.numpy(), columns=frame.index).set_index(frame.index)\n",
    "ne_cov = cov_as_frame.loc[new_england_states, new_england_states]\n",
    "ne_corr = correlation(ne_cov)\n",
    "print(ne_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, these states tend to vote similarly. We can also examine some states of the South which we also expect to be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State        LA        MS        AL        GA        SC\n",
      "State                                                  \n",
      "LA     1.000000  0.554020  0.651511  0.523784  0.517672\n",
      "MS     0.554020  1.000000  0.699459  0.784371  0.769198\n",
      "AL     0.651511  0.699459  1.000000  0.829908  0.723015\n",
      "GA     0.523784  0.784371  0.829908  1.000000  0.852818\n",
      "SC     0.517672  0.769199  0.723015  0.852818  1.000000\n"
     ]
    }
   ],
   "source": [
    "southern_states = ['LA', 'MS', 'AL', 'GA', 'SC']\n",
    "southern_cov = cov_as_frame.loc[southern_states, southern_states]\n",
    "southern_corr = correlation(southern_cov)\n",
    "print(southern_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These correlation matrices show that, as expected, logical groupings of states tend to have similar voting trends. We now look at the correlations *between* the groups (e.g. between Maine and Louisiana)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State        LA        MS        AL        GA        SC\n",
      "State                                                  \n",
      "ME     0.329438  0.309352 -0.000534  0.122375  0.333679\n",
      "VT    -0.036079  0.009653 -0.366604 -0.202065  0.034438\n",
      "NH     0.234105  0.146826 -0.105781  0.008411  0.233084\n",
      "MA     0.338411  0.122257 -0.059107 -0.025730  0.182290\n",
      "RI     0.314088  0.188819 -0.066307 -0.022142  0.186955\n",
      "CT     0.139021  0.074646 -0.205797 -0.107684  0.125023\n"
     ]
    }
   ],
   "source": [
    "cross_cov = cov_as_frame.loc[new_england_states + southern_states, new_england_states + southern_states]\n",
    "cross_corr = correlation(cross_cov)\n",
    "print(cross_corr.loc[new_england_states, southern_states])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we see weaker correlation between New England states and Southern states than the correlation within those grouping. Again, this is as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the expected information gain of a polling strategy\n",
    "The prior we have set up appears to accord, at least approximately, with intuition. However, we now want to add a second source of information from polling. We aim to use our prior to select a polling strategy that will be most informative about our target $w$. A polling strategy, in this simplified set-up, is the number of people to poll in each state. (We ignore any other covariates such as regional variation inside states, demographics, etc.) We might imagine that polling 1000 people in Florida (the most marginal state), will be much more effective than polling 1000 people in DC (the least marginal state). That's because the outcome in DC is already quite predictable, just based on our prior, whereas the outcome in Florida is really up for grabs.\n",
    "\n",
    "In fact, the information that our model will gain about $w$ based on conducting a poll with design $d$ and getting outcome $y$ can be described mathematically as follows:\n",
    "\n",
    "$$\\text{IG}(d, y) = KL(p(w|y,d)||p(w)).$$\n",
    "\n",
    "Since the outcome of the poll is at present unknown, we consider the expected information gain [1]\n",
    "\n",
    "$$\\text{EIG}(d) = \\mathbb{E}_{p(y|d)}\\quad[KL(p(w|y,d)||p(w))].$$\n",
    "\n",
    "### Variational estimators of EIG\n",
    "\n",
    "In the [working memory tutorial](working_memory.ipynb), we used the 'marginal' estimator to find the EIG. This involved estimating the marginal density $p(y|d)$. In this experiment, that would be relatively difficult: $y$ is 51-dimensional with some rather tricky constraints that make modelling its density difficult. Furthermore, the marginal estimator requires us to know $p(y|w)$ analytically, which we do not.\n",
    "\n",
    "Fortunately, other variational estimators of EIG exist: see [2] for more details. One such variational estimator is 'posterior' estimator, based on the following representation\n",
    "\n",
    "$$\\text{EIG}(d) = \\max_q \\mathbb{E}_{p(w, y|d)}\\quad\\left[\\log q(w|y) \\right] + H(p(w)).$$\n",
    "\n",
    "Here, $H(p(w))$ is the prior entropy on $w$ (we can compute this quite easily). The important term involves the variational approximation $q(w|y)$. This $q$ performs amortized variational inference. Specifically, it takes as input $y$ and outputs a distribution over $w$. The bound is maximised when $q(w|y) = p(w|y)$ [2]. Since $w$ is a binary random variable, we can think of $q$ as a classifier that tries to decide, based on the poll outcome, who the eventual winner of the election will be. In this notebook, $q$ will be a neural classifier. Training a neural classifier is a fair bit easier than learning the marginal density of $y$, so we adopt this method to estimate the EIG in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class OutcomePredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(51, 256)\n",
    "        self.lin2 = nn.Linear(256, 256)\n",
    "        self.lin3 = nn.Linear(256, 256)\n",
    "        self.lin4 = nn.Linear(256, 1)\n",
    "        \n",
    "    def compute_dem_probability(self, y):\n",
    "        y = nn.functional.relu(self.lin1(y))\n",
    "        y = nn.functional.relu(self.lin2(y))\n",
    "        y = nn.functional.relu(self.lin3(y))\n",
    "        return self.lin4(y)\n",
    "    \n",
    "    def forward(self, y_dict, design, observation_labels, target_labels):\n",
    "\n",
    "        pyro.module(\"posterior_guide\", self)\n",
    "        \n",
    "        y = y_dict[\"y\"]\n",
    "        dem_prob = self.compute_dem_probability(y).squeeze()\n",
    "        pyro.sample(\"w\", dist.Bernoulli(logits=dem_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use this to compute the EIG for several possible polling strategies. First, we need to compute the $H(p(w))$ term in the above formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(p):\n",
    "    return -p*p.log() - (1-p)*(1-p).log()\n",
    "\n",
    "prior_entropy = h(prior_w_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider four simple polling strategies.\n",
    " 1. Poll 1000 people in Florida only\n",
    " 2. Poll 1000 people in DC only\n",
    " 3. Poll 1000 people spread evenly over the US\n",
    " 4. Using a polling allocation that focuses on swing states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "poll_in_florida = torch.zeros(51)\n",
    "poll_in_florida[9] = 1000\n",
    "\n",
    "poll_in_dc = torch.zeros(51)\n",
    "poll_in_dc[8] = 1000\n",
    "\n",
    "uniform_poll = (1000 // 51) * torch.ones(51)\n",
    "\n",
    "swing_score = 1. / (.5 - torch.tensor(prior_prob_dem.values).squeeze()).abs()\n",
    "swing_poll = 1000 * swing_score / swing_score.sum()\n",
    "swing_poll = swing_poll.round()\n",
    "\n",
    "poll_strategies = OrderedDict([(\"Florida\", poll_in_florida),\n",
    "                               (\"DC\", poll_in_dc),\n",
    "                               (\"Uniform\", uniform_poll),\n",
    "                               (\"Swing\", swing_poll)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now compute the EIG for each option. Since this requires training the network (four times) it may take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Florida 0.25844964385032654\n",
      "DC 0.002807796001434326\n",
      "Uniform 0.27910932898521423\n",
      "Swing 0.2271537184715271\n"
     ]
    }
   ],
   "source": [
    "from pyro.contrib.oed.eig import posterior_eig\n",
    "from pyro.optim import Adam\n",
    "\n",
    "eigs = {}\n",
    "\n",
    "for strategy, allocation in poll_strategies.items():\n",
    "    print(strategy, end=\" \")\n",
    "    guide = OutcomePredictor()\n",
    "    pyro.clear_param_store()\n",
    "    ape = posterior_eig(model, allocation, \"y\", \"w\", 10, 10000, guide, \n",
    "                        Adam({\"lr\": 0.0005}), eig=False, final_num_samples=10000)\n",
    "    eigs[strategy] = prior_entropy - ape\n",
    "    print(eigs[strategy].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the experiment\n",
    "\n",
    "We could continue to try other strategies to see if we can increase the EIG further. However, in this tutorial we will stop here and imagine that we conduct the poll using the best strategy given so far.\n",
    "\n",
    "Having conducted the experiment, there are two ways to process the result. For the most accurate result, we could run Bayesian inference (for instance, using Hamiltonian Monte Carlo) to take samples of the posterior over $\\alpha$. We can use this to sample the posterior over $w$, since $w$ is a deterministic function of $\\alpha$. However, for an approximation, we know that $q(w|y)\\approx p(w|y)$, i.e. running $y$ through the network should give a good approximation of the posterior over $w$.\n",
    "\n",
    "We adopt the second approach here. We sample a number of polling outcomes using the probabilities implied by the actual results from the 2016 election. We then examine the test accuracy: how often would $q$ have been able to predict the outcome correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we retrain $q$ with the chosen polling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eig = 0\n",
    "best_allocation = None\n",
    "for strategy in poll_strategies.keys():\n",
    "    if eigs[strategy] > best_eig:\n",
    "        best_eig = eigs[strategy]\n",
    "        best_allocation = poll_strategies[strategy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = OutcomePredictor()\n",
    "pyro.clear_param_store()\n",
    "posterior_eig(model, best_allocation, \"y\", \"w\", 10, 20000, guide, \n",
    "              Adam({\"lr\": 0.0005}), eig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $\\alpha$ implied by the 2016 results is computed in the same way we computed the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5916, -0.3386, -0.0756, -0.5878,  0.6690,  0.1074,  0.2876,  0.2411,\n",
      "         3.1014, -0.0248, -0.1065,  0.7282, -0.7681,  0.3648, -0.4104, -0.2032,\n",
      "        -0.4520, -0.6487, -0.4126,  0.0639,  0.5761,  0.6037, -0.0047,  0.0333,\n",
      "        -0.3677, -0.3979, -0.4519, -0.5558,  0.0518,  0.0079,  0.2935,  0.1866,\n",
      "         0.4799, -0.0762, -0.8382, -0.1711, -0.8144,  0.2475, -0.0150,  0.3355,\n",
      "        -0.3007, -0.6620, -0.5591, -0.1891, -0.5059,  0.6272,  0.1132,  0.3552,\n",
      "        -0.9525, -0.0163, -1.1366])\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_pickle(\"us_presidential_election_data_test.pickle\")\n",
    "results_2016 = torch.tensor(test_data.values, dtype=torch.float)\n",
    "true_alpha = torch.log(results_2016[..., 0] / results_2016[..., 1])\n",
    "print(true_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider running our chosen polling strategy. We perform 10000 re-runs to account for the randomness in the poll outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioned_model = pyro.condition(model, data={\"alpha\": true_alpha})\n",
    "y, _, _ = conditioned_model(uniform_poll.expand(10000, 51))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under these 10000 hypothetical outcomes of running that same poll, how many times would our $q$ network have gone on to predict the correct electoral outcome?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1295)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(guide.compute_dem_probability(y) < 0.).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that with just 1000 accurate poll records, we may have begun to predict the outcome of the 2016 election with some confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this tutorial, we showed how optimal experimental design principles can be applied to electoral polling. Of course, our model is not realistic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Chaloner, K. and Verdinelli, I., 1995. **Bayesian experimental design: A review.** Statistical Science, pp.273-304.\n",
    "\n",
    "[2] Foster, A., Jankowiak, M., Bingham, E., Horsfall, P., Teh, Y.W., Rainforth, T. and Goodman, N., 2019. **Variational Bayesian Optimal Experimental Design.** Advances in Neural Information Processing Systems 2019 (to appear)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
